#' LinkExtractor
#'
#' A function that take a _charachter_ url as input, fetches its html document, and extract all links following a set of rules.
#' @param url character, url to fetch and extract links.
#' @param id  numeric, an id to identify a specific web page in a website collection, it's auto-generated by default
#' @param lev numeric, the depth level of the web page, auto-generated by the \code{Rcrawler} function.
#' @param IndexErrPages character vector, vector of html error code-statut to process, by default it's c(200),eg to include 404 and 403 pages c(404,403)
#' @param Useragent , default to "Rcrawler"
#' @param Timeout ,default to 5s
#' @param URLlenlimit interger, the url character length limit to index, default to 255 characters (to avoid spider traps)
#' @param urlExtfilter character vector, the list of file extensions to exclude from indexing, by dfault a large list is defined (html pages only are permitted) in order to prevent large files downloading; To define your own use c(ext1,ext2,ext3 ...)
#' @param statslinks  boolean, specifies if input and output links shoud be counted, work only when the function is called from the main function \code{scrawler}
#' @param urlbotfiler character vector , directories/files restricted by robot.txt
#' @param encod character, specify the encoding of th web page
#' @param removeparams character vector, list of url parameters to be removed/ignored
#' @return return a list of two elements, the first is a list containing the web page details (url, encoding-type, content-type, content ... etc), the second is a character-vector containing the list of retreived urls.
#' @author salim khalil
#' @import httr xml2
#' @examples
#'
#' pageinfo<-LinkExtractor(url="http://www.glofile.com")
#'
#' @export

LinkExtractor <- function(url, id, lev, IndexErrPages, Useragent, Timeout=5, URLlenlimit=255,
                          urlExtfilter, statslinks=FALSE, encod, urlbotfiler, removeparams) {
  nblinks<-0
  pageinfo<-list()
  if (missing(removeparams)) removeparams<-""
  if (missing(urlbotfiler)) urlbotfiler<-" "
  if (missing(id)) id<-sample(1:1000, 1)
  if (missing(lev)) lev<-sample(1:1000, 1)
  if (missing(IndexErrPages)) errstat<-c(200)
  else errstat<-c(200,IndexErrPages)
  if(missing(Useragent)) Useragent="Mozilla/5.0 (Windows NT 6.3; WOW64; rv:42.0) Gecko/20100101 Firefox/42.0"
  if(missing(urlExtfilter)) urlExtfilter<-c("flv","mov","swf","txt","xml","js","css","zip","gz","rar","7z","tgz","tar","z","gzip","bzip","tar","mp3","mp4","aac","wav","au","wmv","avi","mpg","mpeg","pdf","doc","docx","xls","xlsx","ppt","pptx","jpg","jpeg","png","gif","psd","ico","bmp","odt","ods","odp","odb","odg","odf")

  page<-tryCatch(GET(url, user_agent(Useragent),timeout(Timeout)) , error=function(e) NULL)
  # 1 if domain exist (could resolve host name)
  if (!is.null(page)){
    # 2 if page exist (not 404,301,302,500,503,403)
    if(page$status_code %in% errstat){
      # 4 if page content is html
      if(grepl("html",page$headers$`content-type`)){
        if (missing(encod)){
        x<-as.character(content(page, type = "htmlTreeParse", as="text"))
        cont<-x
        } else {
        x<-as.character(content(page, type = "htmlTreeParse", as="text", encoding = encod))
        cont<-x
        }
        x<-read_html(x)
        links<-xml2::xml_find_all(x, "//a/@href")
        links<-as.vector(paste(links))
        links<-gsub(" href=\"(.*)\"", "\\1", links)
        links<-unique(links)
        links2 <- vector()
        domain0 <- strsplit(gsub("http://|https://|www\\.", "", url), "/")[[c(1, 1)]]
        domain <- paste(domain0, "/", sep="")
        # Link canonicalization
        links<-LinkNormalization(links,url)
        # Ignore Url parameters
        links<-sapply(links , function(x) Linkparamsfilter(x, removeparams), USE.NAMES = FALSE)
        links<-unique(links)
        # Link robots.txt filter
        if (!missing(urlbotfiler)){
       links<-links[!links %like% paste(urlbotfiler,collapse="|") ]
        }
        if(length(links)!=0) {
          for(s in 1:length(links)){
            if (!is.na(links[s])){
              #limit length URL to 255
              if( nchar(links[s])<=URLlenlimit) {
                ext<-tools::file_ext(sub("\\?.+", "", basename(links[s])))
                # 6 Filtre eliminer les liens externes , le lien source lui meme, les lien avec diese et les liens deja dans dans liste ( evite double), les types de fichier filtrer, les lien tres longs , les liens de type share
                if(grepl(domain,links[s]) && !(url==links[s]) && !(links[s] %in% links2) && !(ext %in% urlExtfilter) && sum(gregexpr("http://", links[s], fixed=TRUE)[[1]]>0)==1){
                  links2<-c(links2,links[s])
                #calcul de nombre des liens OUT
                  nblinks<-nblinks+1
                  }
                #calcul de nombre des liens IN
                if (statslinks){
                if(links[s] %in% pkg.env$shema$urls){
                index<-pkg.env$shema[grep(paste("^",links[s],"$", sep=""),pkg.env$shema$urls),1]
                pkg.env$shema$inn[as.numeric(index)]<-as.numeric(pkg.env$shema$inn[as.numeric(index)])+1}
                }
              }
            }
          }
        } else links2 <- vector()
      } else {links2 <- vector()
              cont<-"NULL"}
    } else {links2 <- vector()
            cont<-"NULL"}
    #Ligne - page detail
    if (!statslinks){
    pageinfo<-list(id,url,"finished",lev,nblinks,"",page$status_code,gsub("(.*)\\;.*", "\\1", page$headers$`content-type`),gsub(".*\\;.", "\\1", page$headers$`content-type`),cont)
    } else {
    pageinfo<-list(id,url,"finished",lev,nblinks,pkg.env$shema$inn[id],page$status_code,gsub("(.*)\\;.*", "\\1", page$headers$`content-type`),gsub(".*\\;.", "\\1", page$headers$`content-type`),cont)

      }

    } else {
      links2 <- vector()
      pageinfo<-list(id,url,"NULL",lev,"","","","","")
            }
  paquet<-list(pageinfo,links2)
  return(paquet)
}

