% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/robotparser.R
\name{RobotParser}
\alias{RobotParser}
\title{RobotParser fetch and parse robots.txt}
\usage{
RobotParser(website, useragent)
}
\arguments{
\item{website}{character, url of the website which rules have to be extracted  .}

\item{useragent}{character, the useragent of the crawler}
}
\value{
return a list of three elements, the first is a character vector of Disallowed directories, the third is a Boolean value which is TRUE if the user agent of the crawler is blocked.
}
\description{
This function fetch and parse robots.txt file of the website which is specified in the first argument and return the list of correspending rules .
}
\examples{

RobotParser("http://www.glofile.com","AgentX")
#Return robot.txt rules and check whether AgentX is blocked or not.


}
